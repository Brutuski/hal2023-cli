#!/usr/bin/env bash

# Colors
# Used to decorate the replies, queries, error messages etc. used in the script
BLACK='\033[0;30m'
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[0;33m'
BLUE='\033[0;34m'
PURPLE='\033[0;35m'
CYAN='\033[0;36m'
WHITE='\033[0;37m'
RESET='\033[0m'

# Logo
# Used only for decorative purposes
readonly ASCII=$(
  cat <<EOF
██╗  ██╗ █████╗ ██╗         ██████╗  ██████╗ ██████╗ ██████╗
██║  ██║██╔══██╗██║         ╚════██╗██╔═████╗╚════██╗╚════██╗
███████║███████║██║          █████╔╝██║██╔██║ █████╔╝ █████╔╝
██╔══██║██╔══██║██║         ██╔═══╝ ████╔╝██║██╔═══╝  ╚═══██╗
██║  ██║██║  ██║███████╗    ███████╗╚██████╔╝███████╗██████╔╝
╚═╝  ╚═╝╚═╝  ╚═╝╚══════╝    ╚══════╝ ╚═════╝ ╚══════╝╚═════╝
EOF
)

# Labels
# Decorative line, shown before the answer. It is shown in the following format:
#<NEWLINE>
# ChatGPT ｢<Model Name> | Tokens Used - <Total Tokens Used>｣ (For text based queries)
# ChatGPT ｢Image｣: <Success Message> (For image generation)
# "ChatGPT" shown in bold and cyan. Model Name and Tokens Used are shown in lighter greyed out text.
# CHATGPT - Used for gpt-3.5-turbo model
# CHATGPT_DAVINCI - Used for text-davinci-003 model
# CHATGPT_IMAGE - Used for image generation
readonly CHATGPT="${CYAN}\033[1mChatGPT\033[0m${RESET} \033[2m｢Turbo Model | Tokens Used -\033[0m"
readonly CHATGPT_DAVINCI="${CYAN}\033[1mChatGPT\033[0m${RESET} \033[2m｢Davinci Model | Tokens Used -\033[0m:"
readonly CHATGPT_IMAGE="${CYAN}\033[1mChatGPT\033[0m${RESET} \033[2m｢Image｣\033[0m:"
readonly CHATGPT_GPT4="${CYAN}\033[1mChatGPT\033[0m${RESET} \033[2m｢GPT4 Model | Tokens Used -\033[0m:"

# API Key
# Located at: ~/.chat-gpt-api
# The key is stored unencrypted, in plain text with no spaces etc.
readonly API_KEY=$(<~/.chat-gpt-api)

# Default Parameters
TEMP=${TEMP:-0.8}
MAX_TOKENS=${MAX_TOKENS:-2048}
RESOLUTION=${RESOLUTION:-1024x1024}

# Date & Time
# Used in the history file
readonly DATE_TIME=$(date +"%d-%m-%y at %H:%M")

# Logo printed when the script is executed
printf "\033c"
echo -e "\n${YELLOW}${ASCII}${RESET}\n"

# Checks for ChatGPT Error Message
# response: Response from curl, without jq
# error: To check if response has .error in JSON
# error_message: Specific error message form OpenAI extracted
function connection_error() {
  local response=$1
  local error
  local error_message

  error=$(echo "$response" | jq -r '.error')
  error_message=$(echo "$response" | jq -r '.error.message')

  if [ "$error" != "null" ]; then
    echo -e "\n${RED}Error:${RESET} Your message failed to reach Open AI's API."
    echo -e "${RED}Error Message:${RESET}"
    echo -e "${YELLOW}${error_message}${RESET}\n"
    exit 1
  fi
}

# Parses response from chatGPT using the gpt-3.5-turbo model
# input: User input
# model: ChatGPT Model to be used in the curl request
# response: Response from curl, without jq
# response_parsed: 'response' parsed for message from ChatGPT
# tokens_used: `response` parsed to extract the total number of tokens used for the query
function chat_response() {
  local input=$1
  local model="gpt-3.5-turbo"
  local response
  local response_parsed
  local tokens_used

  response=$(curl -sS https://api.openai.com/v1/chat/completions \
    -H "Authorization: Bearer $API_KEY" \
    -H "Content-Type: application/json" \
    -d '{
          "model": "'"$model"'",
          "messages": [
                        {"role": "user", "content": "'"$input"'"}
                      ],
          "max_tokens": '$MAX_TOKENS',
          "temperature": '$TEMP'
          }')

  response_parsed=$(echo "$response" | jq -r '.choices[].message.content')
  tokens_used=$(echo "$response" | jq -r '.usage.total_tokens')

  connection_error "$response"
  echo -e "\n${CHATGPT} \033[2m${tokens_used}｣\033[0m:\n${GREEN}${response_parsed}${RESET}"
  save_history "$input" "$response_parsed" "$model" "$tokens_used"
}

# Parses response from chatGPT using the text-davinci-003 model
# input: User input
# model: ChatGPT Model to be used in the curl request
# response: Response from curl, without jq
# response_parsed: 'response' parsed for message from ChatGPT
# tokens_used: `response` parsed to extract the total number of tokens used for the query
function chat_response_davinci() {
  local input=$1
  local model="text-davinci-003"
  local response
  local response_parsed
  local tokens_used

  response=$(curl -sS https://api.openai.com/v1/completions \
    -H 'Content-Type: application/json' \
    -H "Authorization: Bearer $API_KEY" \
    -d '{
    			"model": "'"$model"'",
    			"prompt": "'"$input"'",
    			"max_tokens": '$MAX_TOKENS',
    			"temperature": '$TEMP'
  	}')

  response_parsed=$(echo "$response" | jq -r '.choices[].text' | sed '1,2d')
  tokens_used=$(echo "$response" | jq -r '.usage.total_tokens')

  connection_error "$response"
  echo -e "\n${CHATGPT_DAVINCI} \033[2m${tokens_used}｣\033[0m:\n${GREEN}${response_parsed}${RESET}"
  save_history "$input" "$response_parsed" "$model" "$tokens_used"
}

# Generates image
# input: User input
# success_message: String to be displayed along with the URL
# model: ChatGPT Model to be used in the curl request
# response: Response from curl, without jq
# image_url: 'response' parsed for the image URL
# url_short: Shortens `image_url` using tny.im API
function image_generation() {
  local input=$1
  local success_message="URL was successfully generated."
  local model="DALL.E"
  local response

  response=$(curl -sS https://api.openai.com/v1/images/generations \
    -H 'Content-Type: application/json' \
    -H "Authorization: Bearer $API_KEY" \
    -d '{
          "prompt": "'"$input"'",
          "n": 1,
          "size": "'"$RESOLUTION"'"
    }')

  local image_url
  image_url=$(echo $response | jq -r '.data[0].url')
  local url_short
  url_short=$(wget -qO- -U Mozilla http://tinyurl.com/api-create.php?url=$image_url)

  if [ "$image_url" == null ]; then
    connection_error "$response"
  else
    echo -e "\n${CHATGPT_IMAGE} ${GREEN}Image was successfully generated${RESET}"
    echo -e "Link: $url_short"
    save_history "$input" "$success_message" "$model" "NA"
  fi
}

# Parses response from chatGPT using the gpt-4 model
# input: User input
# model: ChatGPT Model to be used in the curl request
# response: Response from curl, without jq
# response_parsed: 'response' parsed for message from ChatGPT
# tokens_used: `response` parsed to extract the total number of tokens used for the query
function chat_response_gpt4() {
  local input=$1
  local model="gpt-4"
  local response
  local response_parsed
  local tokens_used

  response=$(curl -sS https://api.openai.com/v1/chat/completions \
    -H "Authorization: Bearer $API_KEY" \
    -H "Content-Type: application/json" \
    -d '{
          "model": "'"$model"'",
          "messages": [
                        {"role": "user", "content": "'"$input"'"}
                      ],
          "max_tokens": '$MAX_TOKENS',
          "temperature": '$TEMP'
          }')

  response_parsed=$(echo "$response" | jq -r '.choices[].text' | sed '1,2d')
  tokens_used=$(echo "$response" | jq -r '.usage.total_tokens')

  connection_error "$response"
  echo -e "\n${CHATGPT_GPT4} \033[2m${tokens_used}｣\033[0m:\n${GREEN}${response_parsed}${RESET}"
  save_history "$input" "$response_parsed" "$model" "$tokens_used"
}

# Saves responses in a history file with timestamp
# Located at: ~/.hal2023_history.txt
# History file isn't deleted while uninstalling
# prompt: User input
# reply: Parsed reply from ChatGPT to the user input
# model: Model used for the query
# tokens_used: Tokens used for the text based query. Image generation does not display the total number of tokens used
function save_history() {
  local prompt=$1
  local reply=$2
  local model=$3
  local tokens_used=$4

  # For each individual query, the above variables are stored in the following format:
  # <NEWLINE>
  # +++++++++++++++++ (used for decorative purposes)
  # <NEWLINE>
  # <NEWLINE>
  # dd-mm-yy at HH:MM
  # User: <User query>
  # <NEWLINE>
  # ChatGPT [<Model>] [Tokens Used - <Tokens>] (For image generation tokens used is displayed as NA)
  # <NEWLINE>
  # <ChatGPT Reply> (For image generation "URL was successfully generated" is displayed"
  echo -e "\n+++++++++++++++++\n\n$DATE_TIME\n\nUser: $prompt \n\nChatGPT [$model] [Tokens Used - $tokens_used]\n$reply" >>~/.hal2023_history.txt
}

# Checks for a History File during script run
# If History file is not found, it is created
if [ ! -f ~/.hal2023_history.txt ]; then
  touch ~/.hal2023_history.txt
fi

# Endless `while` loop that runs until the user types in one of the exit keywords
# By default `gpt-3.5-turbo` model is used to reply to queries
# "-davinci" keyword before a prompt points the query to the `text-davinci-003` model via the chat_response_davinci function
# "-image" keyword before a prompt points the query to the `DALL.E` model for image generation via the image_generation function
while true; do
  echo -e "\n\033[2mType exit, quit or :q to exit.\033[0m"
  echo -e "${WHITE}Write a message...${RESET}"
  read -r input

  if [ "$input" == "exit" ] || [ "$input" == "quit" ] || [ "$input" == ":q" ]; then
    echo -e "\nSuccessfuly quit HAL2023\n"
    exit 0
  elif [[ "$input" == "-davinci"* ]]; then
    davinciQuery=$(echo "$input" | cut -c 9-)
    chat_response_davinci "$davinciQuery"
  elif [[ "$input" == "-image"* ]]; then
    image_generation "${input#*-image }"
  elif [[ "$input" == "-gpt4"* ]]; then
    chat_response_gpt4 "${input#*-gpt4 }"
  else
    chat_response "$input"
  fi
done
